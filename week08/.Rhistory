x <- 0
while(x < 10){
paste("x is : ", x)
x <- x + 1
}
while(x < 10){
print('hello')
x <- x + 1
}
while(x < 10){
print('hello')
x <- x + 1
}
while(x < 10){
print('hello')
x <- x + 1
}
x <- 0
while(x < 10){
print('hello')
x <- x + 1
}
x <- 0
x <- 0
while(x < 10){
print(paste('x is : ', x))
x <- x + 1
}
x <- 0
while(x < 10){
paste('x is : ', x)
x <- x + 1
}
x <- 0
while(x < 10){
print(paste('x is : ', x))
x <- x + 1
}
x <- 0
while(x < 10){
print(paste('x is : ', x))
x <- x + 1
}
x <- 0
while(x < 10){
print(paste('x is : ', x))
x <- x + 1
}
x <- 0
while(x < 10){
print(paste('x is : ', x))
}
while(x < 10){
if(x == 3){
print("Bye")
break
}
print(paste('x is : ', x))
x <- x + 1
}
vec <- 1:5
vec
for(data in vec){
print(data + 10)
}
mat <- matrix(1:25, nrow = 5)
mat
for( num in mat){
print(num + 10)
}
for( i in 2:9 ){
for ( j in 1:9 ){
print(paste(i, 'X', j , '=', i*j))
}
}
readline(prompt = "단을 입력하세요. : ")
step <- readline(prompt = "단을 입력하세요. : ")
step * 1
step <- as.numeric(step)
for(i in 1:9){
print( paste(step, "X", i, "=", step*i)  )
}
for(i in 1:9){
print('hello')
print( paste(step, "X", i, "=", step*i)  )
}
for(i in 1:9){
print('hello') print( paste(step, "X", i, "=", step*i)  )
}
for(i in 1:9){
print( paste(step, "X", i, "=", step*i)  )
}
step <- readline(prompt = "단을 입력하세요. : ")
step <- as.numeric(step)
for(i in 1:9){
print( paste(step, "X", i, "=", step*i)  )
}
dataset <- read.csv('Social_Network_Ads.csv')
setwd("~/GitHub/radsp/week08")
dataset <- read.csv('Social_Network_Ads.csv')
dataset <- dataset[ , 3:5]
# Y값
dataset$Purchased <- factor(dataset$Purchased,
levels = c(0,1))
library(caTools)
set.seed(123)
split <- sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set <- subset(dataset, split == T)
test_set <- subset(dataset, split == F)
training_set[ , -3] <- scale(training_set[ , -3])
test_set[ , -3] <- scale(test_set[ , -3])
library(e1071)
colnames(training_set)
classifier <- svm(formula = Purchased ~ Age+EstimatedSalary,
data = training_set,
kernel = 'radial') # rbf커널 : 가우시안 커널
y_pred <- predict(classifier, newdata = test_set[ , -3])
cm <- table(test_set$Purchased, y_pred)
cm
(58+32) / sum(cm)
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Kernel SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
dataset <- read.csv('Social_Network_Ads.csv')
dataset <- dataset[ , 3:5]
dataset$Purchased <- factor(dataset$Purchased,
levels = 0:1)
library(caTools)
set.seed(123)
split <- sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set <- subset(dataset, split == T)
source('~/GitHub/radsp/week08/4_1_decision_tree_classification.R', encoding = 'UTF-8', echo=TRUE)
library(caTools)
set.seed(123)
split <- sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set <- subset(dataset, split == T)
test_set <- subset(dataset, split == F )
install.packages('rpart')
library(rpart)
classifier <- rpart(formula = Purchased ~ . ,
ddata = training_set)
classifier <- rpart(formula = Purchased ~ . ,
data = training_set)
y_pred <- predict(classifier,
newdata = test_set[ , -3],
type = 'class')
y_pred
cm = table(test_set$Purchased, y_pred)
cm
(56+30)/sum(cm)
dataset <- read.csv('Social_Network_Ads.csv')
dataset <- dataset[ , 3:5]
dataset$Purchased <- factor(dataset$Purchased,
levels = 0:1)
library(caTools)
set.seed(123)
split <- sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set <- subset(dataset, split == T)
test_set <- subset(dataset, split == F )
training_set[ , -3] <- scale(training_set[ , -3])
test_set[ , -3] <- scale(test_set[ , -3])
install.packages('rpart')
library(rpart)
classifier <- rpart(formula = Purchased ~ . ,
data = training_set)
y_pred <- predict(classifier,
newdata = test_set[ , -3],
type = 'class')
y_pred
cm = table(test_set$Purchased, y_pred)
cm
install.packages("rpart")
(53 + 30) / sum(cm)
install.packages("rpart")
cm = table(test_set$Purchased, y_pred)
cm
(53 + 30) / sum(cm)
(56+30)/sum(cm)
(53 + 30) / sum(cm)
plot(classifier)
text(classifier)
dataset <- read.csv('Mall_Customers.csv')
View(dataset)
summary(dataset)
dataset[ dataset$Annual.Income..k.. == 137, ]
df <- dataset[ , 4:5]
set.seed(123)
ret.kmeans <- kmeans(x = df,
centers = 5)
y_kmeans <- ret.kmeans$cluster
y_kmeans
dataset <- cbind(dataset, y_kmeans)
View(dataset)
dataset[ dataset$y_kmeans == 3, ]
library(cluster)
clusplot(df,
y_kmeans,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
clusplot(df[ , c(1,2)],
y_kmeans,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
clusplot(df,
y_kmeans,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
set.seed(6)
wcss = c()
# k를 1부터 10까지 설정하고,
# 해당 k의 wcss 값을 구하여, wcss벡터에 저장할 것입니다.
# 그리고 나서 그래프로 표시.
wcss = c()
for( i in 1:10) {
ret.kmeans <- kmeans(df, centers = i)
wcss[i] <- sum(ret.kmeans$withinss)
}
wcss
# 엘보우 메소드를 이용해서, 최적의 k값을 찾는다.
plot(1:10,
wcss,
type = 'b',
main = 'The Elbow Method',
xlab = 'Number of clusters (K)',
ylab = 'WCSS')
ret.kmeans <- kmeans(x = df, centers = 6)
y_kmeans <- ret.kmeans$cluster
y_kmeans
clusplot(df,
y_kmeans,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
ret.kmeans <- kmeans(x = df, centers = 5)
y_kmeans <- ret.kmeans$cluster
y_kmeans
clusplot(df,
y_kmeans,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
dataset <- read.csv('Mall_Customers.csv')
df <- dataset[ , 4:5]
dendrogram <- hclust(d = dist(df, method = 'euclidean'),
method = 'ward.D')
plot(dendrogram,
main = "Dendrogram",
xlab = 'Customers',
ylab = 'Euclidean Distances')
y_pred <- cutree(dendrogram, 5)
y_pred
result <- cbind(dataset, y_pred)
View(result)
library(cluster)
clusplot(df,
y_pred,
lines = 0,
shade = T,
color = T,
labels = 2,
plotchar = F,
span = T,
main = 'Cluseters of Customers',
xlab = 'Annual Income',
ylab = 'Spending Score')
df1 <- read.csv('winequality-red.csv', sep = ';')
View(df1)
df2 <- read.csv('winequality-white.csv', sep = ';')
View(df2)
wine <- rbind(df1, df2)
install.packages('ggplot2')
library(ggplot2)
View(wine)
df1$label <- 'red'
df2$label <- 'white'
wine <- rbind(df1, df2)
df1$label <- 'red'
df2$label <- 'white'
wine <- rbind(df1, df2)
library(ggplot2)
pl <- ggplot(wine, aes(x=residual.sugar)) +
geom_histogram(aes(fill=label), color='black', bins = 50)
pl
summary(wine$residual.sugar)
pl + scale_fill_manual(values = c('#ae4554','#faf7ea'))
pl + scale_fill_manual(values = c('#ae4554','#faf7ea')) +
theme_bw()
pl2 <- ggplot(wine, aes(x=alcohol)) +
geom_histogram(aes(fill=label), color='black', bins=50)+
scale_fill_manual(values = c('#ae4554','#faf7ea')) +
theme_bw()
pl2
pl2 <- ggplot(wine, aes(x=critirc.acid)) +
geom_histogram(aes(fill=label), color='black', bins=50)+
scale_fill_manual(values = c('#ae4554','#faf7ea')) +
theme_bw()
pl2
pl2 <- ggplot(wine, aes(x=citric.acid)) +
geom_histogram(aes(fill=label), color='black', bins=50)+
scale_fill_manual(values = c('#ae4554','#faf7ea')) +
theme_bw()
pl2
clus.data <- wine[  , -13]
wine.cluster <- kmeans(clus.data, centers = 2)
wine.cluster$centers
wine.cluster$cluster
wine <- cbind(wine, pred.group = wine.cluster$cluster)
View(wine.cluster)
View(wine)
table(wine$label, wine.cluster$cluster)
(4/5)/(4/6)
1/ (4/7)
install.packages('arules')
library(arules)
dataset <- read.csv('Market_Basket_Optimisation.csv')
dataset <- read.csv('Market_Basket_Optimisation.csv')
View(dataset)
# 스파스 매트릭스로 데이터를 읽어온다.
dataset <- read.transactions('Market_Basket_Optimisation.csv',
sep=',' ,
rm.duplicates = TRUE)
# 스파스 매트릭스로 데이터를 읽어온다.
dataset <- read.transactions('Market_Basket_Optimisation.csv',
sep=',' ,
rm.duplicates = TRUE)
View(dataset)
dataset <- read.csv('Market_Basket_Optimisation.csv')
# 스파스 매트릭스로 데이터를 읽어온다.
dataset <- read.transactions('Market_Basket_Optimisation.csv',
sep=',' ,
rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 20)
rules <- apriori(data = dataset,
parameter = list(support = 0.003,
confidence = 0.8))
rules <- apriori(data = dataset,
parameter = list(support = 0.003,
confidence = 0.4))
# 실제 규칙을 보는 함수
sorted.rules <- sort(rules, by='lift')
inspect( sorted.rules[1:10] )
rules <- apriori(data = dataset,
parameter = list(support = 0.004,
confidence = 0.2))
# 정렬 후, 실제 규칙을 보는 함수
sorted.rules <- sort(rules, by='lift')
inspect( sorted.rules[1:10] )
rules <- apriori(data = dataset,
parameter = list(support = 0.004,
confidence = 0.3))
# 정렬 후, 실제 규칙을 보는 함수
sorted.rules <- sort(rules, by='lift')
inspect( sorted.rules[1:10] )
rules <- apriori(data = dataset,
parameter = list(support = 0.004,
confidence = 0.4))
# 정렬 후, 실제 규칙을 보는 함수
sorted.rules <- sort(rules, by='lift')
inspect( sorted.rules[1:10] )
rules <- apriori(data = dataset,
parameter = list(support = 0.004,
confidence = 0.3))
# 정렬 후, 실제 규칙을 보는 함수
sorted.rules <- sort(rules, by='lift')
inspect( sorted.rules[1:10] )
(3/4)  /  (3/6)
